{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"web-scraper.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"12SScIL8Wc654loVYcpU9gK0vqSnco9OG","authorship_tag":"ABX9TyMQY1/OiPdHLLRR2O2G87iw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Web Scraper for exported data\n","\n","Made by Artem Yushko"],"metadata":{"id":"ChO3AoHaXB9C"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SZvJUKwWzuG","executionInfo":{"status":"ok","timestamp":1657802878430,"user_tz":240,"elapsed":3590,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"27513538-18e6-481e-c4bf-c338e2a89907"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"]}],"source":["# the God's library, just like regex for data cleaners\n","!pip install beautifulsoup4"]},{"cell_type":"code","source":["# all the imports we will need\n","from bs4 import BeautifulSoup\n","import requests\n","import math\n","import os"],"metadata":{"id":"sbP6r-PNXG85","executionInfo":{"status":"ok","timestamp":1657802878431,"user_tz":240,"elapsed":4,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# retrieving the page links\n","\n","# reading the Telegram channel exports\n","html_docs = []\n","\n","for filename in os.listdir('/content/drive/MyDrive/artem-yushko/data-artem/telegram-exports/Suspilne/'):\n","   with open(os.path.join('/content/drive/MyDrive/artem-yushko/data-artem/telegram-exports/Suspilne/', filename), 'r') as f:\n","       msg = f.read()\n","       html_docs.append(msg)       \n","\n","# extracting the links from there\n","urls = []\n","\n","for doc in html_docs:\n","    soup = BeautifulSoup(doc, 'html.parser')\n","    # getting all the links and moving them to a specific list\n","    for link in soup.find_all('a'):\n","        l = link.get('href')\n","        urls.append(l)\n","\n","# making sure that the links left are only the urls we want\n","urls = [url for url in urls if '//suspilne.media' in url]\n","urls = [url for url in urls if not '@' in url]\n","\n","# removing the duplicate links\n","urls = list(set(urls))\n","\n","print(\"Number of URLs to parse: \" + str(len(urls)))"],"metadata":{"id":"0fec3TdaaO3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set for the lines to be added\n","lines = []\n","\n","# different counters for statistics stuff\n","url_counter = 0\n","result_counter = 0\n","\n","# for url in the list of generater urls\n","for url in urls:\n","    # statistics\n","    if url_counter % 300 == 0:\n","        print(str(math.floor((url_counter/len(urls))*100)) + ' % of the urls are processed. ' + str(result_counter) + ' lines were added.')\n","    # following the link\n","    # r = requests.get(url)\n","    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}) # uncomment if 403 or 429 emerge way too often\n","    url_counter = url_counter + 1\n","    if r.status_code == 403 or r.status_code == 429:\n","        print(\"The website thinks you're DDOSing it!\")\n","    # if the link is valid\n","    if r.status_code == 200:\n","        # get the content\n","        p = r.content\n","        # most of the stuff we need comes in <p> tags, so we'll focus on them\n","        soup_p = BeautifulSoup(p, 'html5lib')\n","        text = soup_p.find_all('p')\n","        # for each text object we obtained\n","        for s in text:\n","            # splitting them into lines (not sentences)\n","            t = s.get_text().split('\\n')\n","            # if the sentence is not empty and finishes on a period (a nice way to catch the artifacts)\n","            if t[0] and t[0][-1] == '.':\n","                # then append it to the list\n","                lines.append(t[0])\n","                result_counter = result_counter + 1\n","    else:\n","      print(f\"Unexpected error! This is the status code: {r.status_code}\")"],"metadata":{"id":"yjItZB4qa107"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(lines))\n","print(lines[7])"],"metadata":{"id":"twocEVqUtheN","executionInfo":{"status":"ok","timestamp":1657777626855,"user_tz":240,"elapsed":5,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff8739c1-f25b-46c0-f0e1-9f9bcfbdec3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["84002\n","Нардеп Тарас Козак перебуває в Росії, а нардеп Віктор Медведчук, у якого у вівторок провели обшуки, знаходиться в Україні.\n"]}]},{"cell_type":"code","source":["text = '\\n'.join(lines)\n","output_path = \"/content/drive/MyDrive/artem-yushko/data-artem/raw/Suspilne.txt\"\n","with open(output_path, 'w') as f:\n","  f.write(text)\n","\n","import re\n","print(len(re.findall('\\.', text)))"],"metadata":{"id":"bnSBowBySN0C","executionInfo":{"status":"ok","timestamp":1657777627249,"user_tz":240,"elapsed":397,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"09ca85c6-f458-4105-c6b6-01f2add85b54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["229423\n"]}]}]}